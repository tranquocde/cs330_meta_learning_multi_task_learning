{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlkUWhvSu-ja"
      },
      "source": [
        "Author: Kyle Hsu\n",
        "\n",
        "With revisions from: Rafael Rafailov, Evan Liu, Fahim Tajwar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI43aS5mRO1s"
      },
      "source": [
        "# Introduction\n",
        "Welcome to the PyTorch tutorial for CS330! This colab notebook accompanies [these slides](https://docs.google.com/presentation/d/1e_md1C24vZsMNtbAJyNyDkau99owklm65IG9MudizvE/edit?usp=sharing). If you haven't already, enable a GPU for this colab instance by doing \"Edit\" -> \"Notebook settings\" -> \"Hardware accelerator\" drop-down -> \"GPU\" -> \"Save\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7cLA7izR_zl"
      },
      "source": [
        "Let's make sure we're using the right Python and PyTorch versions, and that we have a GPU at our disposal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I12wdApTZC_H",
        "outputId": "bbca1417-06c7-43ec-8507-38138ef4953f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version info: 3.7.15 (default, Oct 12 2022, 19:14:55) \n",
            "[GCC 7.5.0]\n",
            "PyTorch version info: 1.12.1+cu113\n",
            "PyTorch detects a GPU: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f'Python version info: {sys.version}')\n",
        "print(f'PyTorch version info: {torch.__version__}')\n",
        "print(f'PyTorch detects a GPU: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptKO3cRFTjWo"
      },
      "source": [
        "# Overview\n",
        "We'll begin by doing some MNIST classification, starting from low-level operations and gradually replacing chunks using `torch` abstractions such as `torch.nn.Module`, `torch.optim.SGD`, and `torch.utils.data.DataLoader`. (Much of this content is adapted from [this tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard.) We'll then proceed to a whirlwind tour over PyTorch features that we suspect will be relevant for your homeworks for this course. Finally, we'll see some tips and tricks for debugging and getting help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t8qKU2yVE5Q"
      },
      "source": [
        "# PyTorch Basics via MNIST\n",
        "### MNIST\n",
        "Download MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VB4quTfVHCb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"./data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7K4tLsIVURi"
      },
      "source": [
        "Load MNIST into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzxqdhhKVTMA"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "        ((x_train, y_train), (x_val, y_val), _) = pickle.load(f, encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouKpbI6DWYWx"
      },
      "source": [
        "The data currently exists as NumPy arrays. Let's take a closer look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "EG9KpubhWbNZ",
        "outputId": "163247fc-539a-4cd5-bdbf-73f73a28230c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training inputs: shape (50000, 784), dtype float32\n",
            "training outputs: shape (50000,), dtype int64\n",
            "input range: (0.0, 0.99609375)\n",
            "label range: (0, 9)\n",
            "label of training example 42: 7\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(f'training inputs: shape {x_train.shape}, dtype {x_train.dtype}')\n",
        "print(f'training outputs: shape {y_train.shape}, dtype {y_train.dtype}')\n",
        "print(f'input range: {x_train.min(), x_train.max()}')\n",
        "print(f'label range: {y_train.min(), y_train.max()}')\n",
        "\n",
        "i_example = 42\n",
        "x_train_example = x_train[i_example].reshape((28, 28))\n",
        "y_train_example = y_train[i_example]\n",
        "plt.imshow(x_train_example, cmap='gray')\n",
        "print(f'label of training example {i_example}: {y_train_example}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COOGtiMZXqYq"
      },
      "source": [
        "We need to convert the data into PyTorch tensors. For ease of use, tensors have some familiar attributes and methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEk34MTIVfh2",
        "outputId": "adf98c71-493a-489b-aa32-28541d7854dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training inputs: shape torch.Size([50000, 784]), dtype torch.float32\n",
            "training outputs: shape torch.Size([50000]), dtype torch.int64\n",
            "input range: (tensor(0.), tensor(0.9961))\n",
            "label range: (tensor(0), tensor(9))\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train, x_val, y_val = map(torch.tensor, (x_train, y_train, x_val, y_val))\n",
        "print(f'training inputs: shape {x_train.shape}, dtype {x_train.dtype}')\n",
        "print(f'training outputs: shape {y_train.shape}, dtype {y_train.dtype}')\n",
        "print(f'input range: {x_train.min(), x_train.max()}')\n",
        "print(f'label range: {y_train.min(), y_train.max()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuLs5f2bcwXU"
      },
      "source": [
        "### Bare Metal MNIST\n",
        "We will first train a model for classifying MNIST digits with nothing but tensor and gradient operations. We'll later substitute out chunks of this code with the appropriate PyTorch abstractions.\n",
        "\n",
        "To keep things simple, for this we'll use a linear model. For a single example, the model is\n",
        "$$ z = W^\\top x + b $$\n",
        "where $W \\in \\mathbb{R}^{784 \\times 10}$ and $b \\in \\mathbb{R}^{10}$ are the parameters to be learned, $x \\in [0, 1]^{784}$ is the MNIST image, and $z \\in \\mathbb{R}^{10}$ are the logits of the categorical distribution for the label of $x$.\n",
        "\n",
        "To obtain categorical parameters from the logits, we do\n",
        "$$ p = \\text{softmax}(z) = \\frac{1}{\\sum_j \\exp(z_j)} \\exp(z).$$\n",
        "\n",
        "The label $y \\in \\{0, \\dots, 9\\}$ implicitly defines a one-hot categorical parameter vector $y' \\in \\{0, 1\\}^{10}$, where $y$ corresponds to the index of $y'$ that is 1. The negative log-likelihood is then\n",
        "$$ \\text{nll}(p, y) = - \\sum_{i=0}^{9} y'_i \\log(p_i) = -\\log(p_y)$$\n",
        "\n",
        "Note that our implementation will assume batched inputs with a batch size of $B$. That is,\n",
        "$$x_b = \\begin{bmatrix} x_1^\\top \\\\ \\vdots \\\\ x_B^\\top \\end{bmatrix} \\in [0, 1]^{B \\times 784}.$$\n",
        " We can vectorize the above operations to process all $B$ inputs at once (naively using `for` loops is much, much slower). For example, to compute the logits, we have\n",
        "$$z_b = x_b W + \\begin{bmatrix} b^\\top \\\\ \\vdots \\\\ b^\\top \\end{bmatrix} \\in \\mathbb{R}^{B \\times 10}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZEHIaizc3CU"
      },
      "outputs": [],
      "source": [
        "W = torch.randn(784, 10, requires_grad=True)\n",
        "b = torch.randn(10, requires_grad=True)\n",
        "\n",
        "def logits(x_b, W, b):\n",
        "    return x_b @ W + b    # addition is broadcasted\n",
        "\n",
        "def log_softmax(z_b):\n",
        "    return z_b - torch.logsumexp(z_b, dim=1, keepdim=True)  # combining log and softmax is more numerically stable\n",
        "\n",
        "def negative_log_likelihood(logp_b, y_b):\n",
        "    return -torch.mean(logp_b[range(y_b.shape[0]), y_b])   # indexing trick\n",
        "\n",
        "def accuracy(logit_b, y_b):\n",
        "    return (logit_b.argmax(dim=1) == y_b).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQYMuIF7i87C"
      },
      "source": [
        "Note that the `requires_grad` keyword argument/attribute determines whether a tensor is a constant or a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vC_MuGOjDTn",
        "outputId": "77d6e6a9-5814-409f-9b58-c9414a84ad6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(b.requires_grad)\n",
        "print(y_train.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrS8cR3OiARG"
      },
      "source": [
        "### Computation Graph\n",
        "Now we have the opportunity to look at some of PyTorch's core mechanics: reverse-mode automatic differentiation (a.k.a. backpropagation) on a dynamic computation graph. Let's take a batch of training data and compute the average loss over the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGQELJgRg_TY"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "x_b, y_b = x_train[:batch_size], y_train[:batch_size]\n",
        "\n",
        "def loss_fn(W, b, x_b, y_b):\n",
        "    logit_b = logits(x_b, W, b)\n",
        "    logp_b = log_softmax(logit_b)\n",
        "    loss = negative_log_likelihood(logp_b, y_b)\n",
        "    return loss\n",
        "\n",
        "loss = loss_fn(W, b, x_b, y_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YpM1c2sj-GN"
      },
      "source": [
        "Computations like the above are automagically recorded on a computation graph. Let's take a peek."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iab2ejfRlpLv",
        "outputId": "e972f9dd-42bb-452c-cce6-d7301ae35f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.1+cu113)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.1.1)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=ec3b0798fbcebeadd2a52a4c706116dd0f44af1cd4aa56ac656665f887fc087b\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "5zVVH_dkkaeq",
        "outputId": "b6158524-e4b0-4914-fcbb-736ec3fee5d1"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
              " -->\n",
              "<!-- Title: %3 Pages: 1 -->\n",
              "<svg width=\"222pt\" height=\"558pt\"\n",
              " viewBox=\"0.00 0.00 222.00 558.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 554)\">\n",
              "<title>%3</title>\n",
              "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-554 218,-554 218,4 -4,4\"/>\n",
              "<!-- 140386900956848 -->\n",
              "<g id=\"node1\" class=\"node\">\n",
              "<title>140386900956848</title>\n",
              "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"132.5,-31 78.5,-31 78.5,0 132.5,0 132.5,-31\"/>\n",
              "<text text-anchor=\"middle\" x=\"105.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n",
              "</g>\n",
              "<!-- 140386896245968 -->\n",
              "<g id=\"node2\" class=\"node\">\n",
              "<title>140386896245968</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"150,-86 61,-86 61,-67 150,-67 150,-86\"/>\n",
              "<text text-anchor=\"middle\" x=\"105.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">NegBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896245968&#45;&gt;140386900956848 -->\n",
              "<g id=\"edge12\" class=\"edge\">\n",
              "<title>140386896245968&#45;&gt;140386900956848</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M105.5,-66.9688C105.5,-60.1289 105.5,-50.5621 105.5,-41.5298\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.0001,-41.3678 105.5,-31.3678 102.0001,-41.3678 109.0001,-41.3678\"/>\n",
              "</g>\n",
              "<!-- 140386896245776 -->\n",
              "<g id=\"node3\" class=\"node\">\n",
              "<title>140386896245776</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"153,-141 58,-141 58,-122 153,-122 153,-141\"/>\n",
              "<text text-anchor=\"middle\" x=\"105.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896245776&#45;&gt;140386896245968 -->\n",
              "<g id=\"edge1\" class=\"edge\">\n",
              "<title>140386896245776&#45;&gt;140386896245968</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M105.5,-121.9197C105.5,-114.9083 105.5,-105.1442 105.5,-96.4652\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.0001,-96.3408 105.5,-86.3408 102.0001,-96.3409 109.0001,-96.3408\"/>\n",
              "</g>\n",
              "<!-- 140386896246032 -->\n",
              "<g id=\"node4\" class=\"node\">\n",
              "<title>140386896246032</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"156,-196 55,-196 55,-177 156,-177 156,-196\"/>\n",
              "<text text-anchor=\"middle\" x=\"105.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">IndexBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896246032&#45;&gt;140386896245776 -->\n",
              "<g id=\"edge2\" class=\"edge\">\n",
              "<title>140386896246032&#45;&gt;140386896245776</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M105.5,-176.9197C105.5,-169.9083 105.5,-160.1442 105.5,-151.4652\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.0001,-151.3408 105.5,-141.3408 102.0001,-151.3409 109.0001,-151.3408\"/>\n",
              "</g>\n",
              "<!-- 140386896246160 -->\n",
              "<g id=\"node5\" class=\"node\">\n",
              "<title>140386896246160</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"150,-251 61,-251 61,-232 150,-232 150,-251\"/>\n",
              "<text text-anchor=\"middle\" x=\"105.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896246160&#45;&gt;140386896246032 -->\n",
              "<g id=\"edge3\" class=\"edge\">\n",
              "<title>140386896246160&#45;&gt;140386896246032</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M105.5,-231.9197C105.5,-224.9083 105.5,-215.1442 105.5,-206.4652\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.0001,-206.3408 105.5,-196.3408 102.0001,-206.3409 109.0001,-206.3408\"/>\n",
              "</g>\n",
              "<!-- 140386896246352 -->\n",
              "<g id=\"node6\" class=\"node\">\n",
              "<title>140386896246352</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"150,-361 61,-361 61,-342 150,-342 150,-361\"/>\n",
              "<text text-anchor=\"middle\" x=\"105.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896246352&#45;&gt;140386896246160 -->\n",
              "<g id=\"edge4\" class=\"edge\">\n",
              "<title>140386896246352&#45;&gt;140386896246160</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M98.3994,-341.9894C92.2964,-333.1791 83.9503,-319.4446 80.5,-306 76.3681,-289.8996 84.0116,-272.3551 91.9055,-259.6342\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"94.8299,-261.5577 97.5329,-251.3135 89.0315,-257.6361 94.8299,-261.5577\"/>\n",
              "</g>\n",
              "<!-- 140386896246544 -->\n",
              "<g id=\"node12\" class=\"node\">\n",
              "<title>140386896246544</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-306 89,-306 89,-287 214,-287 214,-306\"/>\n",
              "<text text-anchor=\"middle\" x=\"151.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">LogsumexpBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896246352&#45;&gt;140386896246544 -->\n",
              "<g id=\"edge11\" class=\"edge\">\n",
              "<title>140386896246352&#45;&gt;140386896246544</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M113.5126,-341.9197C119.8979,-334.2851 129.013,-323.3867 136.7138,-314.1792\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"139.5387,-316.2571 143.2695,-306.3408 134.1691,-311.7662 139.5387,-316.2571\"/>\n",
              "</g>\n",
              "<!-- 140386896245904 -->\n",
              "<g id=\"node7\" class=\"node\">\n",
              "<title>140386896245904</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"92,-416 9,-416 9,-397 92,-397 92,-416\"/>\n",
              "<text text-anchor=\"middle\" x=\"50.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n",
              "</g>\n",
              "<!-- 140386896245904&#45;&gt;140386896246352 -->\n",
              "<g id=\"edge5\" class=\"edge\">\n",
              "<title>140386896245904&#45;&gt;140386896246352</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M60.0803,-396.9197C67.8707,-389.1293 79.0595,-377.9405 88.3824,-368.6176\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"91.063,-370.8868 95.6592,-361.3408 86.1132,-365.937 91.063,-370.8868\"/>\n",
              "</g>\n",
              "<!-- 140386896245712 -->\n",
              "<g id=\"node8\" class=\"node\">\n",
              "<title>140386896245712</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-477 0,-477 0,-458 101,-458 101,-477\"/>\n",
              "<text text-anchor=\"middle\" x=\"50.5\" y=\"-465\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
              "</g>\n",
              "<!-- 140386896245712&#45;&gt;140386896245904 -->\n",
              "<g id=\"edge6\" class=\"edge\">\n",
              "<title>140386896245712&#45;&gt;140386896245904</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-457.9688C50.5,-449.5131 50.5,-436.8901 50.5,-426.2615\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-426.1656 50.5,-416.1656 47.0001,-426.1657 54.0001,-426.1656\"/>\n",
              "</g>\n",
              "<!-- 140386901076976 -->\n",
              "<g id=\"node9\" class=\"node\">\n",
              "<title>140386901076976</title>\n",
              "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"89,-550 12,-550 12,-519 89,-519 89,-550\"/>\n",
              "<text text-anchor=\"middle\" x=\"50.5\" y=\"-526\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (784, 10)</text>\n",
              "</g>\n",
              "<!-- 140386901076976&#45;&gt;140386896245712 -->\n",
              "<g id=\"edge7\" class=\"edge\">\n",
              "<title>140386901076976&#45;&gt;140386896245712</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-518.9604C50.5,-509.6356 50.5,-497.6748 50.5,-487.6317\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-487.35 50.5,-477.3501 47.0001,-487.3501 54.0001,-487.35\"/>\n",
              "</g>\n",
              "<!-- 140386896246672 -->\n",
              "<g id=\"node10\" class=\"node\">\n",
              "<title>140386896246672</title>\n",
              "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"211,-416 110,-416 110,-397 211,-397 211,-416\"/>\n",
              "<text text-anchor=\"middle\" x=\"160.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
              "</g>\n",
              "<!-- 140386896246672&#45;&gt;140386896246352 -->\n",
              "<g id=\"edge8\" class=\"edge\">\n",
              "<title>140386896246672&#45;&gt;140386896246352</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M150.9197,-396.9197C143.1293,-389.1293 131.9405,-377.9405 122.6176,-368.6176\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.8868,-365.937 115.3408,-361.3408 119.937,-370.8868 124.8868,-365.937\"/>\n",
              "</g>\n",
              "<!-- 140386901075728 -->\n",
              "<g id=\"node11\" class=\"node\">\n",
              "<title>140386901075728</title>\n",
              "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"187.5,-483 133.5,-483 133.5,-452 187.5,-452 187.5,-483\"/>\n",
              "<text text-anchor=\"middle\" x=\"160.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (10)</text>\n",
              "</g>\n",
              "<!-- 140386901075728&#45;&gt;140386896246672 -->\n",
              "<g id=\"edge9\" class=\"edge\">\n",
              "<title>140386901075728&#45;&gt;140386896246672</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M160.5,-451.791C160.5,-444.0249 160.5,-434.5706 160.5,-426.3129\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.0001,-426.0647 160.5,-416.0648 157.0001,-426.0648 164.0001,-426.0647\"/>\n",
              "</g>\n",
              "<!-- 140386896246544&#45;&gt;140386896246160 -->\n",
              "<g id=\"edge10\" class=\"edge\">\n",
              "<title>140386896246544&#45;&gt;140386896246160</title>\n",
              "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4874,-286.9197C137.1021,-279.2851 127.987,-268.3867 120.2862,-259.1792\"/>\n",
              "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"122.8309,-256.7662 113.7305,-251.3408 117.4613,-261.2571 122.8309,-256.7662\"/>\n",
              "</g>\n",
              "</g>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7faeeb198890>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchviz\n",
        "torchviz.make_dot(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwJ-5hG7mTOL"
      },
      "source": [
        "We can actually inspect tensors to see that each non-leaf variable tensor records its gradient function based on how it was obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbRLFWkWmUY8"
      },
      "outputs": [],
      "source": [
        "print(b.grad_fn)    # leaf variable\n",
        "print(loss.grad_fn)\n",
        "print(x_b.grad_fn)  # not a variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u88oEuRSZAG"
      },
      "source": [
        "### Using `autograd.grad` to Compute Gradients\n",
        "We can now use reverse-mode automatic differentiation to compute the gradient of the loss with respect to our parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4IuGz-vNru5"
      },
      "outputs": [],
      "source": [
        "from torch import autograd\n",
        "loss = loss_fn(W, b, x_b, y_b)\n",
        "W_grad, b_grad = autograd.grad(loss, inputs=(W, b))\n",
        "print(b_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1U8J8ujfoSp"
      },
      "source": [
        "We're now ready to optimize our linear MNIST model. Each iteration consists of the following steps:\n",
        "* Sample a batch of training data.\n",
        "* Compute the logits from the inputs using the model parameters.\n",
        "* Compute a scalar loss from the logits and labels.\n",
        "* Compute the gradient of the loss with respect to the parameters.\n",
        "* Update the model parameters using the gradient.\n",
        "\n",
        "Note that the last step is done in a context manager that disables the construction of the computation graph.\n",
        "\n",
        "To check whether our optimization was successful, we'll look at the accuracy on the training data before and after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iABo_4DPoInh"
      },
      "outputs": [],
      "source": [
        "lr = 0.5\n",
        "num_epochs = 2\n",
        "\n",
        "def train(W, b):\n",
        "    print(f'accuracy before: {accuracy(logits(x_train, W, b), y_train)}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        i_batch_start = 0\n",
        "        while i_batch_start + batch_size < x_train.shape[0]:\n",
        "            x_b = x_train[i_batch_start:i_batch_start + batch_size]\n",
        "            y_b = y_train[i_batch_start:i_batch_start + batch_size]\n",
        "            i_batch_start += batch_size\n",
        "\n",
        "            logit_b = logits(x_b, W, b)\n",
        "            logp_b = log_softmax(logit_b)\n",
        "            loss = negative_log_likelihood(logp_b, y_b)\n",
        "            W_grad, b_grad = autograd.grad(loss, inputs=(W, b))\n",
        "\n",
        "            with torch.no_grad():   # we don't need gradients for this\n",
        "                W -= lr * W_grad\n",
        "                b -= lr * b_grad\n",
        "\n",
        "    print(f'accuracy after: {accuracy(logits(x_train, W, b), y_train)}')\n",
        "\n",
        "W = torch.randn(784, 10, requires_grad=True)\n",
        "b = torch.randn(10, requires_grad=True)\n",
        "train(W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYArnyJ_nDCt"
      },
      "source": [
        "### Using `backward` to Compute Gradients\n",
        "An alternative to using `autograd.grad` is to call `backward()` on the loss tensor. This _accumulates_ gradients of leaf tensors into their `grad` attribute. (Note that with this we don't need to specify the variables we're taking the gradient with respect to, nor do we obtain the gradients as output from the function call.)\n",
        "\n",
        "When we're done using a gradient in a `grad` attribute, we should reset it to 0, else repeated accumulations may occur. We can do this with the `Tensor` method `zero_()`. (Note that the `_` signifies that the operation modifies the tensor in-place.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta5qt6V1kPAE"
      },
      "outputs": [],
      "source": [
        "print('before computing gradients')\n",
        "print(b.grad)\n",
        "\n",
        "loss = loss_fn(W, b, x_b, y_b)\n",
        "loss.backward()\n",
        "print('after forward pass and calling backward()')\n",
        "print(b.grad)\n",
        "\n",
        "loss = loss_fn(W, b, x_b, y_b)\n",
        "loss.backward()\n",
        "print('after another forward pass and calling backward() again')\n",
        "print(b.grad)\n",
        "\n",
        "b.grad.zero_()\n",
        "print('after calling zero_()')\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9gxa-qsSc0i"
      },
      "source": [
        "When should you use `autograd.grad` vs. `backward`? In general, `backward` is more convenient, but there are times (e.g. in Homework 2) when you need more fine-grained control over gradient computation. Let's see what the training code looks like when we use `backward`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYhQfdJDLJyj"
      },
      "outputs": [],
      "source": [
        "def train(W, b):\n",
        "    print(f'accuracy before: {accuracy(logits(x_train, W, b), y_train)}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        i_batch_start = 0\n",
        "        while i_batch_start + batch_size < x_train.shape[0]:\n",
        "            x_b = x_train[i_batch_start:i_batch_start + batch_size]\n",
        "            y_b = y_train[i_batch_start:i_batch_start + batch_size]\n",
        "            i_batch_start += batch_size\n",
        "\n",
        "            logit_b = logits(x_b, W, b)\n",
        "            logp_b = log_softmax(logit_b)\n",
        "            loss = negative_log_likelihood(logp_b, y_b)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                W -= lr * W.grad\n",
        "                b -= lr * b.grad\n",
        "                W.grad.zero_()\n",
        "                b.grad.zero_()\n",
        "\n",
        "    print(f'accuracy after: {accuracy(logits(x_train, W, b), y_train)}')\n",
        "\n",
        "W = torch.randn(784, 10, requires_grad=True)\n",
        "b = torch.randn(10, requires_grad=True)\n",
        "train(W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1MUJ0cfxD58"
      },
      "source": [
        "### `nn.functional`\n",
        "Now, let's refactor and extend our training code by allowing ourselves to use other `torch` modules. We'll first look at `torch.nn`.\n",
        "\n",
        "`nn.functional` (commonly abbreviated as just `F`) includes many useful functions, e.g. loss functions and activation functions. In particular, `F.cross_entropy` combines the `log_softmax` and `negative_log_likelihood` functions into one.\n",
        "\n",
        "If you're used to using TensorFlow's [CategoricalCrossentropy loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy), note that `F.cross_entropy` takes integer labels, not one-hot labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A2M3dY4xpfs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "with torch.no_grad():\n",
        "    logit_b = logits(x_b, W, b)\n",
        "    print(negative_log_likelihood(log_softmax(logit_b), y_b))\n",
        "    print(F.cross_entropy(logit_b, y_b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I91UsJOmyv1b"
      },
      "source": [
        "### `nn.Module`\n",
        "`nn.Module` is a useful class that corresponds loosely to an intuitive notion of a parameterized model. It\n",
        "* registers `nn.Parameter`s, which are essentially variable tensors, as attributes\n",
        "* holds as the `forward` method the model's forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIN1V4b7yvGF"
      },
      "outputs": [],
      "source": [
        "class MNISTLinear(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(784, 10))\n",
        "        self.b = nn.Parameter(torch.randn(10))\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        return x_b @ self.W + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpd724uU3Vkg"
      },
      "source": [
        "Using the `nn.Module` can be very convenient. For example, previously we had to manually zero the `grad` attribute of `W` and `b`. We can now use the `parameters` method, which returns an iterator over module parameters, and the `zero_grad` method, which zeros the `grad` attribute of every module parameter.\n",
        "\n",
        "After substituting code for the model forward pass, the loss computation, the optimization step, and the gradient zeroing, the training code now looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SJlK8y24BHn"
      },
      "outputs": [],
      "source": [
        "def train(model):\n",
        "    print(f'accuracy before: {accuracy(model(x_train), y_train)}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        i_batch_start = 0\n",
        "        while i_batch_start + batch_size < x_train.shape[0]:\n",
        "            x_b = x_train[i_batch_start:i_batch_start + batch_size]\n",
        "            y_b = y_train[i_batch_start:i_batch_start + batch_size]\n",
        "            i_batch_start += batch_size\n",
        "\n",
        "            logit_b = model(x_b)    # an nn.Module is callable\n",
        "            loss = F.cross_entropy(logit_b, y_b)\n",
        "            loss.backward()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    p -= lr * p.grad\n",
        "                model.zero_grad()\n",
        "\n",
        "    print(f'accuracy after: {accuracy(model(x_train), y_train)}')\n",
        "\n",
        "train(MNISTLinear())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czAS_2oo4qrP"
      },
      "source": [
        "`nn` contains many `nn.Module` subclasses that correspond to commonly used chunks of computation. One such \"layer\" is the linear model we've been using! We can replace our `MNISTLinear` class with an `nn.Linear`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed2QkQ768CLI"
      },
      "outputs": [],
      "source": [
        "train(nn.Linear(784, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2qhStDy8lrP"
      },
      "source": [
        "Our optimization went differently in 2 epochs this time around. This is probably because the parameters in `nn.Linear` are [initialized slightly differently](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). It's always important to read documentation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkOLJWAH8zYD"
      },
      "source": [
        "### MNIST MLP\n",
        "What if we want to use a two-layer fully-connected network? `nn.Module`s can register `nn.Module` attributes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60V-yAKM5m00"
      },
      "outputs": [],
      "source": [
        "class MNISTMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(784, 64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        return self.linear2(self.activation(self.linear1(x_b)))\n",
        "\n",
        "train(MNISTMLP())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG2yPUjhYzzx"
      },
      "source": [
        "##Saving/Loading pre-trained weights\n",
        "Sometimes multiple people may have to use the same network. It can take a lot of time for everyone to train their network from scratch. It is environmentally also disastrous as the carbon footprint of training large networks is very huge.\n",
        "\n",
        "It would be better if we could train and save weights for our network, and reuse them later if we need to. PyTorch gives us ways to do this easily!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjZ2iBpbbPDS"
      },
      "outputs": [],
      "source": [
        "network = MNISTMLP()\n",
        "train(network)\n",
        "\n",
        "torch.save(network.state_dict(), \"network.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ew8Ken2b4He"
      },
      "source": [
        "The above code saves the network's weights to the given path, in this case \"network.pt\". Now we will show that loading the model's weight without retraining it from scratch retrieves prior performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4doKE58-cJV3"
      },
      "outputs": [],
      "source": [
        "network = MNISTMLP()\n",
        "print(f'accuracy of initialized network: {accuracy(network(x_train), y_train)}')\n",
        "\n",
        "network.load_state_dict(torch.load(\"network.pt\"))\n",
        "print(f'accuracy after loading weights: {accuracy(network(x_train), y_train)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYXTLH477ooG"
      },
      "source": [
        "### Are You Registered?\n",
        "`nn.Module`s [have some specific rules](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) regarding what attributes are properly registered (i.e. identified as part of the model). We've seen that `nn.Parameter`s and `nn.Module`s are, but most Python objects are not. For example, you might consider it cleaner to implement the `MNISTMLP` this way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sIQ_XOf-NM1"
      },
      "outputs": [],
      "source": [
        "class MNISTMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = [\n",
        "            nn.Linear(784, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        ]\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        for layer in self.layers:\n",
        "            x_b = layer(x_b)\n",
        "        return x_b\n",
        "\n",
        "train(MNISTMLP())\n",
        "print(list(MNISTMLP().parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn3Ogmn4-umv"
      },
      "source": [
        "Uh oh! Even though the `layers` list has `nn.Module`s, it itself wasn't recognized as a registerable attribute. To get around this, we have the `nn.ModuleList`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA21B41t-7fL"
      },
      "outputs": [],
      "source": [
        "class MNISTMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Linear(784, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        for layer in self.layers:\n",
        "            x_b = layer(x_b)\n",
        "        return x_b\n",
        "\n",
        "train(MNISTMLP())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4KUQfu_QW7"
      },
      "source": [
        "### `nn.Sequential`\n",
        "It's pretty common to have a sequence of layers we want to iteratively apply onto an input. We can replace the `for` loop business in the forward pass by using an `nn.Sequential` container, a very commonly used abstraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rooDuNM2_C4S"
      },
      "outputs": [],
      "source": [
        "train(nn.Sequential(\n",
        "    nn.Linear(784, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx745oji_vrm"
      },
      "source": [
        "### `torch.optim`\n",
        "`torch.optim` contains implementations of many common optimization algorithms. When initializing an optimizer instance, we need to pass in the model parameters. Now, instead of manually coding the update rule, we call the optimizer's `step` method. The optimizer can also handle resetting parameter gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tupyWULJ7ZpU"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "def train(model):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f'accuracy before: {accuracy(model(x_train), y_train)}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        i_batch_start = 0\n",
        "        while i_batch_start + batch_size < x_train.shape[0]:\n",
        "            x_b = x_train[i_batch_start:i_batch_start + batch_size]\n",
        "            y_b = y_train[i_batch_start:i_batch_start + batch_size]\n",
        "            i_batch_start += batch_size\n",
        "\n",
        "            logit_b = model(x_b)\n",
        "            loss = F.cross_entropy(logit_b, y_b)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(f'accuracy after: {accuracy(model(x_train), y_train)}')\n",
        "\n",
        "train(MNISTMLP())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7SRGPQQ-VSB"
      },
      "source": [
        "### `torch.utils.data`\n",
        "Notice how we are manually slicing the `x_train` and `y_train` tensors to get batches. There's a more convenient way to do this:\n",
        "`torch.utils.data` features the `Dataset`, `Sampler`, and `DataLoader` abstractions, which are useful for transforming, sampling, and iterating over data. Let's implement a custom `Dataset` subclass for MNIST which we can then use to instantiate a `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wL0iPSSRKpER"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class MNISTDataset(data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "dataset_train = MNISTDataset(x_train, y_train)\n",
        "dataloader_train = data.DataLoader(dataset_train, batch_size=batch_size)\n",
        "\n",
        "def train(model):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f'accuracy before: {accuracy(model(x_train), y_train)}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        for x_b, y_b in dataloader_train:   # dataloaders are iterators\n",
        "            logit_b = model(x_b)\n",
        "            loss = F.cross_entropy(logit_b, y_b)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(f'accuracy after: {accuracy(model(x_train), y_train)}')\n",
        "\n",
        "train(MNISTMLP())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3flJEs4LKZK"
      },
      "source": [
        "### Using a GPU\n",
        "Now, let's see how to use a GPU in PyTorch. Every tensor has a `device` attribute (default: `'cpu'`). GPU-accelerated computation requires the involved tensors to all be on device `'cuda'`. The `to` method is a handy way to move tensors around.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWosupXrT_IV"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(1.)\n",
        "print(x.device)\n",
        "x = x.to('cuda')\n",
        "print(x.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ms19-qYfHB"
      },
      "source": [
        "The `nn.Module` also has a `to` method that transfers all parameters (and buffer items) to a specified device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeokNbD3YDXr"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "\n",
        "dataset_train = data.TensorDataset(x_train, y_train)\n",
        "dataloader_train = data.DataLoader(dataset_train, batch_size=batch_size)\n",
        "\n",
        "def train(model):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f'accuracy before: {accuracy(model(x_train.to(\"cuda\")), y_train.to(\"cuda\"))}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        for x_b, y_b in dataloader_train:\n",
        "            x_b, y_b = x_b.to('cuda'), y_b.to('cuda')\n",
        "            logit_b = model(x_b)\n",
        "            loss = F.cross_entropy(logit_b, y_b)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(f'accuracy after: {accuracy(model(x_train.to(\"cuda\")), y_train.to(\"cuda\"))}')\n",
        "\n",
        "train(MNISTMLP().to('cuda'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKUTPPTVT2e"
      },
      "source": [
        "### Clean-up\n",
        "To wrap this section up, we'll refactor our code to facilitate assessing on both train and val data with data-loading, and using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCtv2mcTAWSF"
      },
      "outputs": [],
      "source": [
        "dataset_val = data.TensorDataset(x_val, y_val)\n",
        "dataloader_val = data.DataLoader(dataset_val, batch_size=batch_size)\n",
        "\n",
        "def loss_fn(model, x_b, y_b):\n",
        "    logit_b = model(x_b)\n",
        "    loss = F.cross_entropy(logit_b, y_b)\n",
        "    return loss, y_b.shape[0]\n",
        "\n",
        "def accuracy(model, x_b, y_b):\n",
        "    logit_b = model(x_b)\n",
        "    accuracy = (logit_b.argmax(dim=1) == y_b).float().mean()\n",
        "    return accuracy, y_b.shape[0]\n",
        "\n",
        "def assess(model, dataloader):\n",
        "    loss_total_r, correct_r, n_r = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x_b, y_b in dataloader:\n",
        "            x_b, y_b = x_b.to('cuda'), y_b.to('cuda')\n",
        "            loss, n = loss_fn(model, x_b, y_b)\n",
        "            acc, n = accuracy(model, x_b, y_b)\n",
        "            loss_total_r += loss * n\n",
        "            correct_r += acc * n\n",
        "            n_r += n\n",
        "    loss_total_r, correct_r = loss_total_r.item(), correct_r.item()\n",
        "    return loss_total_r / n_r, correct_r / n_r\n",
        "\n",
        "def train(model, optimizer):\n",
        "    print('before')\n",
        "    print(f'train loss and acc: {assess(model, dataloader_train)}')\n",
        "    print(f'val loss and acc: {assess(model, dataloader_val)}')\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        for x_b, y_b in dataloader_train:\n",
        "            x_b, y_b = x_b.to('cuda'), y_b.to('cuda')\n",
        "            loss, _ = loss_fn(model, x_b, y_b)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print('after')\n",
        "    print(f'train loss and acc: {assess(model, dataloader_train)}')\n",
        "    print(f'val loss and acc: {assess(model, dataloader_val)}')\n",
        "\n",
        "model = MNISTMLP().to('cuda')\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "train(model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwUn0NLuVjeY"
      },
      "source": [
        "# More PyTorch Features: A Whirlwind Tour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkZ98YSob72X"
      },
      "source": [
        "### MNIST CNN\n",
        "2D convolutional layers expect an input of shape $(B, C, H, W)$ or `(batch, channels, height, width)`. For a batch of MNIST images, this is $(B, 1, 28, 28)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfXPPG3gb_cz"
      },
      "outputs": [],
      "source": [
        "class MNISTCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=20, kernel_size=(5, 5), stride=1, padding=0),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5), stride=1, padding=0),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(800, 500),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(500, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        return self.layers(x_b.view((-1, 1, 28, 28)))\n",
        "\n",
        "model = MNISTCNN().to('cuda')\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLV7dSnk_xC4"
      },
      "source": [
        "### MNIST LSTM\n",
        "When constructed using the `batch_first=True` argument, `nn.LSTM`s expect inputs of shape $(B, L, D)$ or `(batch, sequence length, element size)`. We'll interpret the rows of the MNIST image as sequence elements, and compute the logits from the last LSTM output (i.e. wait for the LSTM to see the entire image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSA3chD5Aiu_"
      },
      "outputs": [],
      "source": [
        "class MNISTLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.LSTM = nn.LSTM(input_size=28, hidden_size=64, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x_b):\n",
        "        out, _ = self.LSTM(x_b.view((-1, 28, 28)))  # out contains outputs at each iteration over the sequence\n",
        "        return self.linear(out[:, -1, :])   # only the out at the last iteration has seen the entire image\n",
        "\n",
        "model = MNISTLSTM().to('cuda')\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gNjS2HEeEo"
      },
      "source": [
        "### `nn.Embedding`\n",
        "Oftentimes we will have categorical data with too many categories to comfortably convert into one-hot encodings. Examples of this include tokens in NLP and $(x,y)$ positions on a grid. One common way to process this data is to use an `nn.Embedding`, or a table of dense vectors that can be indexed by the categorical data. The dense vectors can be optimized. `nn.Embedding` supports advanced indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_45NPu8MHZMu"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(num_embeddings=1000, embedding_dim=64)\n",
        "print(embedding(torch.tensor(42)).shape)\n",
        "print(embedding(torch.tensor(42)).requires_grad)\n",
        "indices = torch.tensor([\n",
        "    [0, 1, 42, 999],\n",
        "    [1, 1, 2, 3]\n",
        "])\n",
        "print(indices.shape)\n",
        "print(embedding(indices).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4hyCvaoIq-c"
      },
      "source": [
        "If you're interested in NLP, a nice use of `nn.Embedding` and recurrent layers (`nn.GRU`) for natural language translation can be found in [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pun7z9zGqbvH"
      },
      "source": [
        "### `torch.distributions`\n",
        "Aside from conveniently providing commonly used methods (e.g. density/mass evaluation), one of the main features of this module is the use of the reparameterization trick to facilitate backpropagation through random samples to the underlying distribution parameters (and beyond)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHyPfnxdJvc4"
      },
      "outputs": [],
      "source": [
        "from torch import distributions\n",
        "loc = torch.tensor(0., requires_grad=True)\n",
        "scale = torch.tensor(1., requires_grad=True)\n",
        "p = distributions.Normal(loc, scale)\n",
        "x = p.rsample(torch.Size([5]))\n",
        "print(x)    # grad_fn comes from reparameterization trick\n",
        "y = -torch.mean(x**2)\n",
        "y.backward()\n",
        "print(scale.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRZRNM6MLZMU"
      },
      "source": [
        "If we don't use `rsample`, we can't take gradients through the sampling step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWDu3fi6LN0y"
      },
      "outputs": [],
      "source": [
        "x = p.sample(torch.Size([5]))\n",
        "print(x)\n",
        "y = -torch.sum(x**2)\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrIbkiJhNyRF"
      },
      "source": [
        "# Debugging, Documentation, and Getting Help\n",
        "Your assignments won't be in Jupyter notebooks, but in Python scripts. This means you won't have the interactivity of cell-based execution, but to the rescue comes `pdb`, the Python Debugger. Use it to\n",
        "\n",
        "*   set breakpoints in the code to interactively inspect program elements: `import pdb; pdb.set_trace()`\n",
        "*   automatically start a debugging session when an exception is thrown: `python -m pdb -c continue main.py`\n",
        "\n",
        "Both will be super, super useful.\n",
        "\n",
        "When something about PyTorch is confusing you, the first place to look is the [PyTorch documentation](https://https://pytorch.org/docs/stable/index.html). (The author of this tutorial consulted the documentation no fewer than 20 times when making it.) This is often the last place you need to look, but sometimes the [PyTorch forums](https://https://discuss.pytorch.org/) can be helpful, too. (Use the search tool!) And of course, you're always welcome to post on Ed or seek out one of your friendly neighborhood CAs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FIR5icqCax"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
